{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQuADのデータ処理\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import pickle\n",
    "import collections\n",
    "\n",
    "def head_find(tgt):\n",
    "    q_head=[\"what\",\"how\",\"who\",\"when\",\"which\",\"where\",\"why\",\"whose\",\"whom\",\"is\",\"are\",\"was\",\"were\",\"do\",\"did\",\"does\"]\n",
    "    tgt_tokens=word_tokenize(tgt)\n",
    "    true_head=\"<none>\"\n",
    "    for h in q_head:\n",
    "        if h in tgt_tokens:\n",
    "            true_head=h\n",
    "            break\n",
    "    return true_head\n",
    "\n",
    "def modify(sentence,question,answer,answer_replace):\n",
    "    head=head_find(question)\n",
    "    \"\"\"\n",
    "    if answer in sentence:\n",
    "        sentence=sentence.replace(answer,\" ans_rep_tag \")\n",
    "    \"\"\"\n",
    "    sentence=\" \".join([sentence,\"ans_pos_tag\",answer,\"inter_tag\",head])\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def c2wpointer(context_text,context,answer_start,answer_end):#answer_start,endをchara単位からword単位へ変換\n",
    "    #nltk.tokenizeを使って分割\n",
    "    #ダブルクオテーションがなぜか変化するので処理\n",
    "    token_id={}\n",
    "    cur_id=0\n",
    "    for i,token in enumerate(context):\n",
    "        start=context_text.find(token,cur_id)\n",
    "        token_id[i]=(start,start+len(token))\n",
    "        cur_id=start+len(token)\n",
    "    for i in range(len(token_id)):\n",
    "        if token_id[i][0]<=answer_start and answer_start<=token_id[i][1]:\n",
    "            answer_start_w=i\n",
    "            break\n",
    "    for i in range(len(token_id)):\n",
    "        if token_id[i][0]<=answer_end and answer_end<=token_id[i][1]:\n",
    "            answer_end_w=i\n",
    "            break\n",
    "    return answer_start_w,answer_end_w\n",
    "\n",
    "#sentenceを受け取り、tokenizeして返す\n",
    "def tokenize(sent):\n",
    "    return [token.replace('``','\"').replace(\"''\",'\"') for token in word_tokenize(sent)]\n",
    "\n",
    "#context_textを文分割して、answer_start~answer_end(char単位)のスパンが含まれる文を返す\n",
    "#やってることはc2iと多分同じアルゴリズム\n",
    "def answer_find(context_text,answer_start,answer_end,answer_replace):\n",
    "    context=sent_tokenize(context_text)\n",
    "    current_p=0\n",
    "    for i,sent in enumerate(context):\n",
    "        end_p=current_p+len(sent)\n",
    "        if current_p<=answer_start and answer_start<=end_p:\n",
    "            sent_start_id=i\n",
    "        if current_p<=answer_end and answer_end<=end_p:\n",
    "            sent_end_id=i\n",
    "        current_p+=len(sent)+1#スペースが消えている分の追加、end_pの計算のところでするべきかは不明\n",
    "    answer_sent=\" \".join(context[sent_start_id:sent_end_id+1])\n",
    "    #ここで答えを置換する方法。ピリオドが消滅した場合などに危険なので止める。\n",
    "    \"\"\"\n",
    "    if answer_replace:\n",
    "        context_text=context_text.replace(context[answer_start:answer_end],\"<answer_word>\")\n",
    "        answer_sent=sent_tokenize(context_text)[sent_start_id]\n",
    "    \"\"\"\n",
    "    return answer_sent\n",
    "\n",
    "\n",
    "def data_process(input_path,src_path,tgt_path,word_count,lower=True):\n",
    "    with open(input_path,\"r\") as f:\n",
    "        data=json.load(f)\n",
    "    contexts=[]\n",
    "    questions=[]\n",
    "    answer_starts=[]\n",
    "    answer_ends=[]\n",
    "    answer_texts=[]\n",
    "    answers=[]\n",
    "    sentences=[]\n",
    "    ids=[]\n",
    "    answer_replace=False\n",
    "    word2count=collections.Counter()\n",
    "    char2count=collections.Counter()\n",
    "    for topic in tqdm(data[\"data\"]):\n",
    "        topic=topic[\"paragraphs\"]\n",
    "        for paragraph in topic:\n",
    "            context_text=paragraph[\"context\"].lower()\n",
    "            for qas in paragraph[\"qas\"]:\n",
    "                question_text=qas[\"question\"].lower()\n",
    "                if len(qas[\"answers\"])==0:\n",
    "                    continue\n",
    "                a=qas[\"answers\"][0]\n",
    "                answer=a[\"text\"].lower()\n",
    "                answer_start=a[\"answer_start\"]\n",
    "                answer_end=a[\"answer_start\"]+len(a[\"text\"])\n",
    "                answer_sent=answer_find(context_text,answer_start,answer_end,answer_replace)#contextの中からanswerが含まれる文を見つけ出す\n",
    "                \"\"\"\n",
    "                answer_sent=\" \".join(tokenize(answer_sent))\n",
    "                question_text=\" \".join(tokenize(question_text))\n",
    "                answer=\" \".join(tokenize(answer))\n",
    "                \"\"\"\n",
    "                answer_sent=modify(answer_sent,question_text,answer,answer_replace)#answwer_sentにanswerを繋げる。\n",
    "                #tokenizeを掛けて処理\n",
    "                answer_sent=\" \".join(tokenize(answer_sent))\n",
    "                question_text=\" \".join(tokenize(question_text))\n",
    "                answer=\" \".join(tokenize(answer))\n",
    "                questions.append(question_text)\n",
    "                sentences.append(answer_sent)\n",
    "\n",
    "    print(len(questions),len(sentences))\n",
    "\n",
    "    with open(src_path,\"w\")as f:\n",
    "        for s in sentences:\n",
    "            f.write(s+\"\\n\")\n",
    "\n",
    "    with open(tgt_path,\"w\")as f:\n",
    "        for s in questions:\n",
    "            f.write(s+\"\\n\")\n",
    "\n",
    "#main\n",
    "version=\"1.1\"\n",
    "type=\"normal\"\n",
    "data_process(input_path=\"data/squad_train-v{}.json\".format(version),src_path=\"data/squad-src-train-{}.txt\".format(type),tgt_path=\"data/squad-tgt-train-{}.txt\".format(type),word_count=True,lower=True)\n",
    "data_process(input_path=\"data/squad_dev-v{}.json\".format(version),src_path=\"data/squad-src-dev-{}.txt\".format(type),tgt_path=\"data/squad-tgt-dev-{}.txt\".format(type),word_count=True,lower=True)\n",
    "\n",
    "#python preprocess.py -train_src data/squad-src-train.txt -train_tgt data/squad-tgt-train.txt -valid_src data/squad-src-val.txt -valid_tgt data/squad-tgt-val.txt -save_data data/demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import pickle\n",
    "import collections\n",
    "\n",
    "input_path=\"data/coqa-train-v1.0.json\"\n",
    "output_path=\"data/coqa-train-v1.0-split1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7199/7199 [00:00<00:00, 914052.02it/s]\n"
     ]
    }
   ],
   "source": [
    "input_path=\"data/coqa-train-v1.0.json\"\n",
    "output_path=\"data/coqa-train-v1.0-split2.json\"\n",
    "\n",
    "with open(input_path)as f:\n",
    "    data=json.load(f)\n",
    "\n",
    "count=0\n",
    "new_data={\"version\": \"1.0\",\"data\": []}\n",
    "for paragraph in tqdm(data[\"data\"]):\n",
    "    if count%2==1:\n",
    "        new_data[\"data\"].append(paragraph)\n",
    "    count+=1\n",
    "    \n",
    "with open(output_path,\"w\")as f:\n",
    "    json.dump(new_data,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coqa-dev.json\n",
      "coqa-src-dev-interro_history.txt\n",
      "coqa-src-dev-normal.txt\n",
      "coqa-src-dev-paragraph_interro.txt\n",
      "coqa-src-train-interro_history.txt\n",
      "coqa-src-train-interro_history2.txt\n",
      "coqa-src-train-normal.txt\n",
      "coqa-src-train-paragraph-interro.txt\n",
      "coqa-tgt-dev-interro_history.txt\n",
      "coqa-tgt-dev-normal.txt\n",
      "coqa-tgt-dev-paragraph_interro.txt\n",
      "coqa-tgt-train-interro_history.txt\n",
      "coqa-tgt-train-interro_history2.txt\n",
      "coqa-tgt-train-normal.txt\n",
      "coqa-tgt-train-paragraph-interro.txt\n",
      "coqa-train.json\n",
      "old\n",
      "pred_coqa_interro_history.txt\n",
      "pred_coqa_paragraph_interro.txt\n",
      "pred_interro.txt\n",
      "pred_interro_answer.txt\n",
      "pred_interro_answer_sub.txt\n",
      "pred_interro_history.txt\n",
      "pred_interro_sub.txt\n",
      "pred_modify.txt\n",
      "pred_normal.txt\n",
      "pred_paragraph_interro.txt\n",
      "pred_paragraph_interro_11.txt\n",
      "pred_sentence_answer.txt\n",
      "pred_sentence_answer2.txt\n",
      "pred_test.txt\n",
      "pred_test2.txt\n",
      "squad-dev-v1.1.json\n",
      "squad-dev-v2.0.json\n",
      "squad-src-dev-interro.txt\n",
      "squad-src-dev-interro_answer.txt\n",
      "squad-src-dev-interro_answer_sub.txt\n",
      "squad-src-dev-interro_history.txt\n",
      "squad-src-dev-interro_sub.txt\n",
      "squad-src-dev-normal.txt\n",
      "squad-src-dev-paragraph_interro.txt\n",
      "squad-src-dev-sentence_answer.txt\n",
      "squad-src-train-interro.txt\n",
      "squad-src-train-interro_answer.txt\n",
      "squad-src-train-interro_history.txt\n",
      "squad-src-train-normal.txt\n",
      "squad-src-train-paragraph_interro.txt\n",
      "squad-src-train-sentence_answer.txt\n",
      "squad-tgt-dev-interro.txt\n",
      "squad-tgt-dev-interro_answer.txt\n",
      "squad-tgt-dev-interro_answer_sub.txt\n",
      "squad-tgt-dev-interro_history.txt\n",
      "squad-tgt-dev-interro_sub.txt\n",
      "squad-tgt-dev-normal.txt\n",
      "squad-tgt-dev-paragraph_interro.txt\n",
      "squad-tgt-dev-sentence_answer.txt\n",
      "squad-tgt-train-interro.txt\n",
      "squad-tgt-train-interro_answer.txt\n",
      "squad-tgt-train-interro_history.txt\n",
      "squad-tgt-train-normal.txt\n",
      "squad-tgt-train-paragraph_interro.txt\n",
      "squad-tgt-train-sentence_answer.txt\n",
      "squad-train-v1.1.json\n",
      "squad-train-v2.0.json\n",
      "squad_test_data.json\n",
      "squad_train_data.json\n",
      "src-dev.txt\n",
      "src-train.txt\n",
      "src-val.txt\n",
      "tgt-dev.txt\n",
      "word2id2vec.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls data/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
