{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQuADのデータ処理\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import pickle\n",
    "import collections\n",
    "\n",
    "def head_find(tgt):\n",
    "    q_head=[\"what\",\"how\",\"who\",\"when\",\"which\",\"where\",\"why\",\"whose\",\"whom\",\"is\",\"are\",\"was\",\"were\",\"do\",\"did\",\"does\"]\n",
    "    tgt_tokens=word_tokenize(tgt)\n",
    "    true_head=\"<none>\"\n",
    "    for h in q_head:\n",
    "        if h in tgt_tokens:\n",
    "            true_head=h\n",
    "            break\n",
    "    return true_head\n",
    "\n",
    "def modify(sentence,question,answer,answer_replace):\n",
    "    head=head_find(question)\n",
    "    \"\"\"\n",
    "    if answer in sentence:\n",
    "        sentence=sentence.replace(answer,\" ans_rep_tag \")\n",
    "    \"\"\"\n",
    "    sentence=\" \".join([sentence,\"ans_pos_tag\",answer,\"inter_tag\",head])\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def c2wpointer(context_text,context,answer_start,answer_end):#answer_start,endをchara単位からword単位へ変換\n",
    "    #nltk.tokenizeを使って分割\n",
    "    #ダブルクオテーションがなぜか変化するので処理\n",
    "    token_id={}\n",
    "    cur_id=0\n",
    "    for i,token in enumerate(context):\n",
    "        start=context_text.find(token,cur_id)\n",
    "        token_id[i]=(start,start+len(token))\n",
    "        cur_id=start+len(token)\n",
    "    for i in range(len(token_id)):\n",
    "        if token_id[i][0]<=answer_start and answer_start<=token_id[i][1]:\n",
    "            answer_start_w=i\n",
    "            break\n",
    "    for i in range(len(token_id)):\n",
    "        if token_id[i][0]<=answer_end and answer_end<=token_id[i][1]:\n",
    "            answer_end_w=i\n",
    "            break\n",
    "    return answer_start_w,answer_end_w\n",
    "\n",
    "#sentenceを受け取り、tokenizeして返す\n",
    "def tokenize(sent):\n",
    "    return [token.replace('``','\"').replace(\"''\",'\"') for token in word_tokenize(sent)]\n",
    "\n",
    "#context_textを文分割して、answer_start~answer_end(char単位)のスパンが含まれる文を返す\n",
    "#やってることはc2iと多分同じアルゴリズム\n",
    "def answer_find(context_text,answer_start,answer_end,answer_replace):\n",
    "    context=sent_tokenize(context_text)\n",
    "    current_p=0\n",
    "    for i,sent in enumerate(context):\n",
    "        end_p=current_p+len(sent)\n",
    "        if current_p<=answer_start and answer_start<=end_p:\n",
    "            sent_start_id=i\n",
    "        if current_p<=answer_end and answer_end<=end_p:\n",
    "            sent_end_id=i\n",
    "        current_p+=len(sent)+1#スペースが消えている分の追加、end_pの計算のところでするべきかは不明\n",
    "    answer_sent=\" \".join(context[sent_start_id:sent_end_id+1])\n",
    "    #ここで答えを置換する方法。ピリオドが消滅した場合などに危険なので止める。\n",
    "    \"\"\"\n",
    "    if answer_replace:\n",
    "        context_text=context_text.replace(context[answer_start:answer_end],\"<answer_word>\")\n",
    "        answer_sent=sent_tokenize(context_text)[sent_start_id]\n",
    "    \"\"\"\n",
    "    return answer_sent\n",
    "\n",
    "\n",
    "def data_process(input_path,src_path,tgt_path,word_count,lower=True):\n",
    "    with open(input_path,\"r\") as f:\n",
    "        data=json.load(f)\n",
    "    contexts=[]\n",
    "    questions=[]\n",
    "    answer_starts=[]\n",
    "    answer_ends=[]\n",
    "    answer_texts=[]\n",
    "    answers=[]\n",
    "    sentences=[]\n",
    "    ids=[]\n",
    "    answer_replace=False\n",
    "    word2count=collections.Counter()\n",
    "    char2count=collections.Counter()\n",
    "    for topic in tqdm(data[\"data\"]):\n",
    "        topic=topic[\"paragraphs\"]\n",
    "        for paragraph in topic:\n",
    "            context_text=paragraph[\"context\"].lower()\n",
    "            for qas in paragraph[\"qas\"]:\n",
    "                question_text=qas[\"question\"].lower()\n",
    "                if len(qas[\"answers\"])==0:\n",
    "                    continue\n",
    "                a=qas[\"answers\"][0]\n",
    "                answer=a[\"text\"].lower()\n",
    "                answer_start=a[\"answer_start\"]\n",
    "                answer_end=a[\"answer_start\"]+len(a[\"text\"])\n",
    "                answer_sent=answer_find(context_text,answer_start,answer_end,answer_replace)#contextの中からanswerが含まれる文を見つけ出す\n",
    "                \"\"\"\n",
    "                answer_sent=\" \".join(tokenize(answer_sent))\n",
    "                question_text=\" \".join(tokenize(question_text))\n",
    "                answer=\" \".join(tokenize(answer))\n",
    "                \"\"\"\n",
    "                answer_sent=modify(answer_sent,question_text,answer,answer_replace)#answwer_sentにanswerを繋げる。\n",
    "                #tokenizeを掛けて処理\n",
    "                answer_sent=\" \".join(tokenize(answer_sent))\n",
    "                question_text=\" \".join(tokenize(question_text))\n",
    "                answer=\" \".join(tokenize(answer))\n",
    "                questions.append(question_text)\n",
    "                sentences.append(answer_sent)\n",
    "\n",
    "    print(len(questions),len(sentences))\n",
    "\n",
    "    with open(src_path,\"w\")as f:\n",
    "        for s in sentences:\n",
    "            f.write(s+\"\\n\")\n",
    "\n",
    "    with open(tgt_path,\"w\")as f:\n",
    "        for s in questions:\n",
    "            f.write(s+\"\\n\")\n",
    "\n",
    "#main\n",
    "version=\"1.1\"\n",
    "type=\"normal\"\n",
    "data_process(input_path=\"data/squad_train-v{}.json\".format(version),src_path=\"data/squad-src-train-{}.txt\".format(type),tgt_path=\"data/squad-tgt-train-{}.txt\".format(type),word_count=True,lower=True)\n",
    "data_process(input_path=\"data/squad_dev-v{}.json\".format(version),src_path=\"data/squad-src-dev-{}.txt\".format(type),tgt_path=\"data/squad-tgt-dev-{}.txt\".format(type),word_count=True,lower=True)\n",
    "\n",
    "#python preprocess.py -train_src data/squad-src-train.txt -train_tgt data/squad-tgt-train.txt -valid_src data/squad-src-val.txt -valid_tgt data/squad-tgt-val.txt -save_data data/demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import pickle\n",
    "import collections\n",
    "\n",
    "input_path=\"data/coqa-train-v1.0.json\"\n",
    "output_path=\"data/coqa-train-v1.0-split1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7199/7199 [00:00<00:00, 914052.02it/s]\n"
     ]
    }
   ],
   "source": [
    "input_path=\"data/coqa-train-v1.0.json\"\n",
    "output_path=\"data/coqa-train-v1.0-split2.json\"\n",
    "\n",
    "with open(input_path)as f:\n",
    "    data=json.load(f)\n",
    "\n",
    "count=0\n",
    "new_data={\"version\": \"1.0\",\"data\": []}\n",
    "for paragraph in tqdm(data[\"data\"]):\n",
    "    if count%2==1:\n",
    "        new_data[\"data\"].append(paragraph)\n",
    "    count+=1\n",
    "    \n",
    "with open(output_path,\"w\")as f:\n",
    "    json.dump(new_data,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist=[]\n",
    "\n",
    "with open(\"data/pred_coqa_split2_interro.txt\")as f:\n",
    "    for line in f:\n",
    "        mylist.append(line.rstrip())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where was the Auction held ?\n",
      "\n",
      "Where was the Auction held ?\n",
      "\n",
      "much did they expected ?\n",
      "\n",
      "history_append_tag much did they expected ?\n",
      "\n",
      "WHo buy the Jackson Glove history_append_tag Where was the seller of the glove from ?\n",
      "\n",
      "Who 's father ?\n",
      "\n",
      "Who 's father ?\n",
      "\n",
      "Why does he different from the other reindeer ?\n",
      "\n",
      "him makes him different from the other reindeer ?\n",
      "\n",
      "he save during a snow storm ?\n",
      "\n",
      "Does What Island does he travel to ?\n",
      "\n",
      "What Island company produced the movie ?\n",
      "\n",
      "pepsi produced the movie ?\n",
      "\n",
      "When did spectre did methods in filming the movie ?\n",
      "\n",
      "filming methods were used in filming the movie ?\n",
      "\n",
      "Are all of the puppets still in existence ?\n",
      "\n",
      "How many remain ?\n",
      "\n",
      "Which Where had they been stored since their use in the show ?\n",
      "\n",
      "since their use in the show ?\n",
      "\n",
      "When were put on display ?\n",
      "\n",
      "were put on display ?\n",
      "\n",
      "much ? much did they appraise for in 2005 ?\n",
      "\n",
      "much did they pay for in 2005 ?\n",
      "\n",
      "much did they originally cost to produce ?\n",
      "\n",
      "What year were made ?\n",
      "\n",
      "Was Budapest always one city ?\n",
      "\n",
      "Was How many was it ?\n",
      "\n",
      "How many was it called ?\n",
      "\n",
      "What was one called ?\n",
      "\n",
      "Where was it located ?\n",
      "\n",
      "What was the other ?\n",
      "\n",
      "Where was it located ?\n",
      "\n",
      "When did they combine ?\n",
      "\n",
      "the city in it 's country ?\n",
      "\n",
      "people live there live there ?\n",
      "\n",
      "Do other people visit ?\n",
      "\n",
      "What do they do ?\n",
      "\n",
      "history_append_tag ? do people like to go ?\n",
      "\n",
      "When do people like to go ?\n",
      "\n",
      "history_append_tag When was LA started ?\n",
      "\n",
      "When was the climate like there ?\n",
      "\n",
      "What is the climate like there ?\n",
      "\n",
      "What is it close to ?\n",
      "\n",
      "How many people live in Taipei ?\n",
      "\n",
      "cheap Do people prefer to buy things in Hong Kong ?\n",
      "\n",
      "who recently heart surgery ? who had heart surgery ?\n",
      "\n",
      "who recommended heart surgery ?\n",
      "\n",
      "what should the driver have done ?\n",
      "\n",
      "did he leave them ?\n",
      "\n",
      "did the driver help ?\n",
      "\n",
      "who tried to bring Nicole back to life ?\n",
      "\n",
      "how old is Ted ?\n",
      "\n",
      "did the driver try to pick more people up ?\n",
      "\n",
      "looking into the situation ?\n",
      "\n",
      "was May heading with Nicole ?\n",
      "\n",
      "what had been put into sasha ?\n",
      "\n",
      "nicole what had recently been put into sasha ?\n",
      "\n",
      "what was the mother 's name ?\n",
      "\n",
      "what is the mother 's name ?\n",
      "\n",
      "When was teh University established ?\n",
      "\n",
      "When was teh University established ?\n",
      "\n",
      "How many professionals is it organized into ?\n",
      "\n",
      "students students ? history_append_tag and in the college ?\n",
      "\n",
      "in the college ?\n",
      "\n",
      "What is the University home to ?\n",
      "\n",
      "Is What will be completed in 2015 ?\n",
      "\n",
      "will completed in 2015 ?\n",
      "\n",
      "will we history_append_tag ? history_append_tag What ?\n",
      "\n",
      "history_append_tag Does the University have a medical school ?\n",
      "\n",
      "Does the University have a medical school ?\n",
      "\n",
      "What age did Einstein start talking ?\n",
      "\n",
      "What age did he start talking ?\n",
      "\n",
      "When did he start reading ?\n",
      "\n",
      "Who was worried about him ?\n",
      "\n",
      "Did Beethoven Thomas Edison 's music teacher support him ?\n",
      "\n",
      "Did Thomas Edison 's teachers think he was smart ?\n",
      "\n",
      "Did Thomas one in particular ?\n",
      "\n",
      "history_append_tag one in particular ?\n",
      "\n",
      "What did he have difficulty with as a youngster ?\n",
      "\n",
      "What did he have difficulty with as a youngster ?\n",
      "\n",
      "What else did he have trouble with ?\n",
      "\n",
      "What Did his father believed he could overcome his difficulties ?\n",
      "\n",
      "Did his father could overcome his difficulties ?\n",
      "\n",
      "anyone agree with his father ?\n",
      "\n",
      "Was it another family member ?\n",
      "\n",
      "Which one ?\n",
      "\n",
      "Did the uncle think he could be taught ?\n",
      "\n",
      "Was Walt Disney a successful newspaper editor ?\n",
      "\n",
      "What did he have ?\n",
      "\n",
      "When was domesticated first domesticated ?\n",
      "\n",
      "When was domesticated first domesticated ?\n",
      "\n",
      "Why about chickens ?\n",
      "\n",
      "What about chickens ?\n",
      "\n",
      "Are today 's cows virtually the same as their ancestors ?\n",
      "\n",
      "What did breeders look for when breeding ?\n",
      "\n",
      "what can one avoid food diarrhea when eating poultry ?\n",
      "\n",
      "How is the term poultry defined ?\n",
      "\n",
      "Are pidgeons considered poultry ?\n",
      "\n",
      "What are sometimes sometimes called ?\n",
      "\n",
      "From which language is \" poultry \" derived ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in mylist[0:100]:\n",
    "    print(i)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
